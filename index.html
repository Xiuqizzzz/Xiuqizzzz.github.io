
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tommy (Xiuqi) Zhu</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<!-- Navigation Bar -->
<nav class="topnav">
  <div class="nav-container">
    <div class="nav-logo">
      <a href="index.html">Tommy Zhu</a>
    </div>
    <div class="nav-links">
      <a href="index.html" class="active">About</a>
      <a href="cv.html">CV <span class="download-icon">üìÑ</span></a>
      <a href="blog.html">Blog</a>
    </div>
  </div>
</nav>

<div class="layout">
  <!-- Sidebar -->
  <div class="sidebar">
    <img src="assets/profile.jpg" class="profile-img" alt="Tommy Zhu" />
    <div class="social-section">
      <a href="https://scholar.google.com/citations?user=YOUR_ID" target="_blank" rel="noopener noreferrer">
        <img src="assets/icon_google.png" class="icon" alt="Google Scholar" />
      </a>
      <a href="https://www.linkedin.com/in/YOUR_LINKEDIN" target="_blank" rel="noopener noreferrer">
        <img src="assets/icon_linkedin.png" class="icon" alt="LinkedIn" />
      </a>
      <a href="https://twitter.com/YOUR_USERNAME" target="_blank" rel="noopener noreferrer">
        <img src="assets/icon_x.png" class="icon" alt="Twitter/X" />
      </a>
      <a href="mailto:zhu.xiuq@northeastern.edu">
        <img src="assets/icon_email.png" class="icon" alt="Email" />
      </a>
    </div>
    
    <!-- News Section -->
    <div class="news-section">
      <div class="news-header">
        <h3 id="news-heading">News</h3>
      </div>

      <div id="news-panel" class="news-scroll" role="region" aria-labelledby="news-heading" data-expanded="false">
        <ul>
          <li>üì¢ 09/2025 ‚Äî Submitted two first-author CHI 2026 full papers on smart glasses & VR for education!</li>
          <li>üì¢ 09/2025 ‚Äî Submitted one TEI 2026 pictorial for my first mentorship experience.</li>
          <li>üì¢ 06/2025 ‚Äî One First-author Paper accepted by IJHCI, good research takes time!</li>
          <li>üì¢ 04/2025 ‚Äî Attending my very second CHI conference in Yokohama!</li>
          <li>üì¢ 02/2025 ‚Äî First-author paper accepted by CHI LBW, see you in Yokohama!</li>
          <li>üìÑ 01/2025 ‚Äî Submitted three papers to DIS, CHI LBW, and IJHCI</li>
        </ul>
      </div>
    </div>
  </div>

<div class="main-content">
      <h1>Tommy (Xiuqi) Zhu</h1>
      <h2>Ph.D. Student @ Northeastern University</h2>

 <p class="bio">
      I am a third-year Ph.D. student in 
      <a href="https://camd.northeastern.edu/" target="_blank">Interdisciplinary Media and Design @ Northeastern CAMD</a>, 
      advised by 
      <a href="https://camd.northeastern.edu/faculty/eileen-mcgivney/" target="_blank">Prof. Eileen McGivney</a>. 
      My research lies at the intersection of <strong>Human-Computer Interaction, Extended Reality (XR), and AI</strong>. 
      Specifically, I design and study <strong>AI-powered smart glasses</strong> that support 
      <em>complex, collaborative, and everyday activities</em>‚Äîenhancing human‚Äìhuman collaboration without disrupting natural social interactions. 
      I have also conducted side research on VR for simulation training, accessible technology for blind and low-vision students, and tangible systems for creativity
      Previously, I interned as a UX Researcher at <a href="https://www.larksuite.com/" target="_blank">ByteDance Lark</a>.  
      I received my B.A. in Digital Media Arts from the <a href="https://www.cuc.edu.cn/" target="_blank">Communication University of China</a>.  
    </p>

<!-- Á†îÁ©∂ÂÖ¥Ë∂£ÈÉ®ÂàÜ -->
<section>
  <h3>Research Interests</h3>
  
  <div class="research-container">
    <!-- ‰∏äÊñπÊñáÊú¨ÈÉ®ÂàÜ -->
    <div class="research-text">
      <p>
        My ongoing research focuses on integrating multimodal large language models (MLLMs) into XR to:
      </p>
      <ol>
        <li>Seamlessly interpret, understand, and respond to users' environments and needs in real-time, enabling XR to support complex, collaborative, and everyday multi-user tasks.</li>
        <li>Transform interactive educational content into dynamic, personalized, and memorable simulation-based learning experiences through AI-driven, adaptive instruction.</li>
      </ol>
    </div>
    
    <!-- ‰∏ãÊñπÂõæÁâáÈÉ®ÂàÜ -->
    <div class="research-image">
      <img src="assets/research-illustration.png" alt="AI-Enhanced XR Educational Experience" />
    </div>
  </div>
</section>


   <section>
  <h3>Publications</h3>
  <p><em>* denotes equal contribution. ‚ÄúXiuqi Zhu‚Äù and ‚ÄúXiuqi Tommy Zhu‚Äù refer to the same author.</em></p>

  <h4>Peer-reviewed Full Conference and Journal Publications</h4>
<div class="pub">
    <img src="assets/ijhci2024_blindcollege.avif" class="pub-img" alt="Blind and Low Vision Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Understanding the Practice, Perception, and Challenge of Blind or Low Vision Students Learning through Accessible Technologies in Non-Inclusive ‚ÄòBlind College‚Äô</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Ziyue Qiu, Ye Wei, Jianhao Wang, Yang Jiao
      </div>
      <em>International Journal of Human Computer Interaction (2025)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>In developing and underdeveloped regions, many 'Blind Colleges' exclusively enroll individuals with Blindness or Vision Impairment (BLV) for higher education. While advancements in accessible technologies have facilitated BLV student integration into 'Integrated Colleges,' their implementation in 'Blind Colleges' remains uneven due to complex economic, social, and policy challenges. This study investigates the practices, perceptions, and challenges of BLV students using accessible technologies in a Chinese 'Blind College' through a two-part empirical approach. Our findings demonstrate that tactile and digital technologies enhance access to education but face significant integration barriers. We emphasize the critical role of early education in addressing capability gaps, BLV students' aspirations for more inclusive educational environments, and the systemic obstacles within existing frameworks. We advocate for leveraging accessible technologies to transition 'Blind Colleges' into 'Integrated Colleges,' offering actionable insights for policymakers, designers, and educators. Finally, we outline future research directions on accessible technology innovation and its implications for BLV education in resource-constrained settings.</p>
      </div>
      <p><a href="https://doi.org/10.1080/10447318.2025.2526569" target="_blank">[DOI]</a></p>
    </div>
  </div>
     
  <div class="pub">
    <img src="assets/ismar2023_moveit.avif" class="pub-img" alt="Can You Move It Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Can You Move It?: The Design and Evaluation of Moving VR Shots in Sports Broadcast</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Zhu</strong>, Cenyi Wang, Zichun Guo, Yifan Zhao, Yang Jiao
      </div>
      <em>IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2023)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>Virtual Reality (VR) broadcasting has seen widespread adoption in major sports events, attributed to its ability to generate a sense of presence, curiosity, and excitement among viewers. However, we have noticed that still shots reveal a limitation in the movement of VR cameras and hinder the VR viewing experience in current VR sports broadcasts. This paper aims to bridge this gap by engaging in a quantitative user analysis to explore the design and impact of dynamic VR shots on viewing experiences. We conducted two user studies in a digital hockey game twin environment and asked participants to evaluate their viewing experience through two questionnaires. Our findings suggested that the viewing experiences demonstrated no notable disparity between still and moving shots for single clips. However, when considering entire events, moving shots improved the viewer‚Äôs immersive experience, with no notable increase in sickness compared to still shots. We further discuss the benefits of integrating moving shots into VR sports broadcasts and present a set of design considerations and potential improvements for future VR sports broadcasting.</p>
      </div>
      <p><a href="https://ieeexplore.ieee.org/abstract/document/10316354" target="_blank">[DOI]</a></p>
    </div>
  </div>

  <h4>Posters, Extended Abstracts, Workshop Papers and Technical Reports</h4>
  <div class="pub">
    <img src="assets/chi2025_vr_simulation.avif" class="pub-img" alt="CHI 2025 VR Simulation Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Designing VR Simulation System for Clinical Communication Training with LLMs-Based Embodied Conversational Agents</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Mingxin Cheng, Heidi Cheerman, Sheri Kiami, Leanne Chukoskie, Eileen McGivney
      </div>
      <em>Conference on Human Factors in Computing Systems (CHILBW 2025)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>VR simulation in Health Professions (HP) education demonstrates huge potential, but fixed learning content with little customization limits its application beyond lab environments. To address these limitations in the context of VR for patient communication training, we conducted a user-centered study involving semi-structured interviews with advanced HP students to understand their challenges in clinical communication training and perceptions of VR-based solutions. From this, we derived design insights emphasizing the importance of realistic scenarios, simple interactions, and unpredictable dialogues. Building on these insights, we developed the Virtual AI Patient Simulator (VAPS), a novel VR system powered by Large Language Models (LLMs) and Embodied Conversational Agents (ECAs), supporting dynamic and customizable patient interactions for immersive learning. We also provided an example of how clinical professors could use user-friendly design forms to create personalized scenarios that align with course objectives in VAPS and discuss future implications of integrating AI-driven technologies into VR education.</p>
      </div>
      <p><a href="https://dl.acm.org/doi/full/10.1145/3706599.3719693">[DOI]</a></p>
    </div>
  </div>

  <div class="pub">
    <img src="assets/idc2023_cospace.avif" class="pub-img" alt="Co-Space Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Co-Space: A Tangible System Supporting Social Attention and Social Behavioral Development through Embodied Play</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Zhu</strong>, Min Fan, Zhuohao Wu, Jiayi Lu, Yukai Liu
      </div>
      <em>Interaction Design Children (IDC 2023)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>Early impairments in social attention prevent children with autism spectrum disorder (ASD) from acquiring social information and disrupt their social behavior development. We present Co-Space, a tangible system that supports children with ASD aged 4-8 years in developing social attention and social behaviors through collaborative and embodied play in a classroom setting. Children can paint tangible hemispheres, insert them on a projection board, and then press or rotate the hemispheres independently or collaboratively to view dynamic audiovisual feedback. We leveraged the strengths of children with ASD (e.g., who may have visual strengths and prefer repetitive actions) by providing them with tangible hemispheres to engage their participation. We utilized codependent design and dynamic audiovisual cues to facilitate children‚Äôs social attention and encourage their collaboration and potential social behaviors. We demonstrated multiple ways to integrate the tangible system into classroom activities. We discuss the benefits of designing tangible systems for supporting social attention and social behaviors for children with ASD through play in the classroom.</p>
      </div>
      <p><a href="https://doi.org/10.1145/3585088.3593911" target="_blank">[DOI]</a></p>
    </div>
  </div>

  <div class="pub">
    <img src="assets/hcii2022_sounds.avif" class="pub-img" alt="Natural Sounds Library Figure" />
    <div class="pub-text">
      <h5 class="pub-title">An Initial Attempt to Build a Natural Sounds Library based on Heuristic Evaluation</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Zhu</strong>, Jingyu Zhang, Tongyang Liu, Gang He
      </div>
      <em>Human Computer Interaction International Poster 2022</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>Attention restoration theory (ART) predicts that the natural environment can restore consumed attentional resources. Previous studies also found presenting natural scenes visually can also have such an effect but whether natural sounds may also have this effect has not been fully examined. In this study, we used an exploratory approach to build a library of natural sounds. We surveyed 204 people by asking them to name ten different types of ‚Äònatural‚Äô sounds and the ten types of ‚Äòrelaxing‚Äô sounds. The collected more than 1,800 answers were then coded according to the source and the characteristics. Finally, twenty-one categories of sounds emerged from these responses. Among them, six categories were considered to be both relaxing and natural (e.g. birds‚Äô songs). For other categories, they were only natural (e.g. thunder) or only relaxing (e.g. music). We discussed how to use this sound library in future studies.</p>
      </div>
      <p><a href="https://doi.org/10.1007/978-3-031-19679-9_90" target="_blank">[DOI]</a></p>
    </div>
  </div>

<h4>Under-review Manuscripts and In Preparation</h4>
      <div class="pub">
        <img src="assets/Figure_Visual Breakdown.png" class="pub-img" alt="Co-Space Figure" />
        <div class="pub-text">
          <h5 class="pub-title">"Reimagining the Future of Smart Glasses Through Exploring the Conversational Successes and Breakdowns in Everyday Activities</h5>
          <div class="pub-authors">
            <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Xiaoan Liu, Casper Harteveld, Smit Desai, Eileen McGivney
          </div>
          <em>Under Review, CHI 2026</em>
          <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
            <p>Smart glasses hold the potential to support everyday activities by combining continuous environmental sensing with voice-only
interaction powered by large language models (LLMs). Understanding how conversational successes and breakdowns in everyday
activities could better guide the design of future smart glasses. To investigate this, we conducted a two-phase study. First, a month-
long collaborative autoethnography (n=2) identified situated patterns of successes and breakdowns with vision-based smart glasses.
Building on these insights, we then engaged eight participants in multi-phase interviews to reflect on ideal roles and envision future
interactions. Our findings reveal recurring breakdowns in intent recognition, perception, and the shared understanding of activity
context. We propose design considerations for future smart glasses, such as situated in context, clear autonomy boundaries, and social
integration. We argue that smart glasses should not be treated merely as interactional novelties, but as opportunities to rethink smart
glasses as a newly built social ecosystem.</p>
          </div>
          <p><a href="https://drive.google.com/file/d/1MfcuygC7BYSNEcPq_5uWAyD-eHdO0iXZ/view?usp=sharing" target="_blank">[PDF]</a></p>
        </div>
      </div>

      <div class="pub">
        <img src="assets/chi2026_VAPS.jpg" class="pub-img" alt="Co-Space Figure" />
        <div class="pub-text">
          <h5 class="pub-title">"It doesn't look real, but it did feel real.": Designing and Evaluating an LLM-Powered VR Simulation for Health Profession Education</h5>
          <div class="pub-authors">
            <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Heidi Cheerman, Mingxin Cheng, Sheri Kiami, Leanne Chukoskie, Eileen McGivney
          </div>
          <em>Under Review, CHI 2026</em>
          <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
            <p>Clinical communication is essential for health professions (HP) students, yet simulation labs and prior VR systems often rely on rigid scripts that fail to capture the unpredictability of real patient encounters. We present VAPS, a Virtual AI Patient Simulator that integrates embodied conversational agents (ECAs) with large language models (LLMs) to support dynamic, voice-based interactions. We conducted a mixed-method evaluation with 40 HP students from five disciplines, examining usability, sense of agency, emotional engagement, and perceived realism. Results show that VAPS was usable, reduced nervousness, and was perceived as immersive and emotionally authentic. Students viewed VAPS as a supplement rather than a replacement for simulation labs and envisioned future extensions including diverse patient cases and interprofessional training. Our findings suggest that VR simulation should evolve from looking real toward fostering experiences that feel real, with conversational AI playing a central role in enabling more adaptive and authentic learning..</p>
          </div>
          <p><a href="https://drive.google.com/file/d/1cSJKZMIYFnKbdXnB990VPkFcNblc4Nhi/view?usp=sharing" target="_blank">[PDF]</a></p>
        </div>
      </div>


     <div class="pub">
        <img src="assets/Figure_Visual Breakdown.png" class="pub-img" alt="Co-Space Figure" />
        <div class="pub-text">
          <h5 class="pub-title">Reshaping Inclusive Interpersonal Dynamics through Smart Glasses in
Mixed-Vision Social Activities</h5>
          <div class="pub-authors">
           Yumo Zhang*, Jieqiong Ding*,  <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Kaige Yang, Yuqing Wei, Shiyi Wang, Yishan Liu, Yang Jiao
          </div>
          <em>Under Review, IUI 2026</em>
          <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
            <p>Meaningful social interaction is vital to well-being, yet Blind and Low Vision (BLV) individuals face persistent barriers when collaborating 
rating with sighted peers due to limited access to visual cues. While most wearable assistive technologies emphasize individual tasks,
smart glasses introduce opportunities for real-time, contextual support in collaborative settings. To explore how smart glasses can
affect the interpersonal dynamics and support inclusive experiences in mixed-vision groups, we developed a smart glasses‚Äìbased system-
tem CollabLens as a technology probe, and employed it in four mixed-vision workshops. Through mixed-method analysis, we found
that smart glasses can meaningfully support inclusive collaboration and strengthen BLV participants‚Äô autonomy in mixed-vision
activities. However, while sighted participants viewed smart glasses as reducing their assistance burden and enhancing mutual understanding, BLV participants primarily valued smart glasses for independent task completion rather than social inclusion, due to
persistent barriers such as technical unreliability, physical discomfort, and social embarrassment. We concluded by discussing and
synthesizing challenges and opportunities for designing smart glasses that foster natural social dynamics in future inclusive settings.</p>
          </div>
          <p><a href="https://drive.google.com/file/d/1MfcuygC7BYSNEcPq_5uWAyD-eHdO0iXZ/view?usp=sharing" target="_blank">[PDF]</a></p>
        </div>
      </div>
    </div>
  </div>
</body>
</html>
