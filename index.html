
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tommy (Xiuqi) Zhu</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<!-- Navigation Bar -->
<nav class="topnav">
  <div class="nav-container">
    <div class="nav-logo">
      <a href="index.html">Tommy Zhu</a>
    </div>
    <div class="nav-links">
      <a href="index.html" class="active">About</a>
      <a href="cv.html">CV <span class="download-icon">üìÑ</span></a>
      <a href="project.html">Projects</a>
      <a href="blog.html">Blog</a>
    </div>
  </div>
</nav>

<div class="layout">
  <!-- Sidebar -->
  <div class="sidebar">
    <img src="Photos/TProfile.JPG" class="profile-img" alt="Tommy Zhu" />
    <div class="social-section">
      <a href="https://scholar.google.com/citations?user=YOUR_ID" target="_blank" rel="noopener noreferrer">
        <img src="assets/icon_google.png" class="icon" alt="Google Scholar" />
      </a>
      <a href="https://www.linkedin.com/in/YOUR_LINKEDIN" target="_blank" rel="noopener noreferrer">
        <img src="assets/icon_linkedin.png" class="icon" alt="LinkedIn" />
      </a>
      <a href="https://twitter.com/YOUR_USERNAME" target="_blank" rel="noopener noreferrer">
        <img src="assets/icon_x.png" class="icon" alt="Twitter/X" />
      </a>
      <a href="mailto:zhu.xiuq@northeastern.edu">
        <img src="assets/icon_email.png" class="icon" alt="Email" />
      </a>
    </div>
    
    <!-- News Section -->
    <div class="news-section">
      <div class="news-header">
        <h3 id="news-heading">News</h3>
      </div>

      <div id="news-panel" class="news-scroll" role="region" aria-labelledby="news-heading" data-expanded="false">
        <ul>
          <li>üì¢ 2/2026 ‚Äî What a new year with 5 submission and passed my QUALIFICATION!</li>
          <li>üì¢ 12/2025 ‚Äî Rejections *4</li>
          <li>üì¢ 09/2025 ‚Äî Submitted two first-author CHI 2026 full papers on smart glasses & VR for education!</li>
          <li>üì¢ 09/2025 ‚Äî Submitted one TEI 2026 pictorial for my first mentorship experience.</li>
          <li>üì¢ 06/2025 ‚Äî One First-author Paper accepted by IJHCI, good research takes time!</li>
          <li>üì¢ 04/2025 ‚Äî Attending my very second CHI conference in Yokohama!</li>
          <li>üì¢ 02/2025 ‚Äî First-author paper accepted by CHI LBW, see you in Yokohama!</li>
          <li>üìÑ 01/2025 ‚Äî Submitted three papers to DIS, CHI LBW, and IJHCI</li>
        </ul>
      </div>
    </div>
  </div>

<div class="main-content">
      <h1>Tommy (Xiuqi) Zhu</h1>
      <h2>Ph.D. Student @ Northeastern University</h2>

 <p class="bio">
      I am a third-year Ph.D. student in 
      <a href="https://camd.northeastern.edu/" target="_blank">Interdisciplinary Media and Design @ Northeastern CAMD</a>, 
      advised by 
      <a href="https://camd.northeastern.edu/faculty/eileen-mcgivney/" target="_blank">Prof. Eileen McGivney</a>. 
      My research lies at the intersection of <strong>Human-Computer Interaction, Extended Reality (XR), and AI</strong>. 
      Specifically, I design and study <strong>AI-powered smart glasses</strong> that support 
      <em>complex, collaborative, and everyday activities</em>‚Äîenhancing human‚Äìhuman collaboration without disrupting natural social interactions. 
      I have also conducted side research on VR for simulation training, accessible technology for blind and low-vision students, and tangible systems for creativity
      Previously, I interned as a UX Researcher at <a href="https://www.larksuite.com/" target="_blank">ByteDance Lark</a>.  
      I received my B.A. in Digital Media Arts from the <a href="https://www.cuc.edu.cn/" target="_blank">Communication University of China</a>.  
    </p>

<!-- Á†îÁ©∂ÂÖ¥Ë∂£ÈÉ®ÂàÜ -->
<section>
  <h3>Research Interests</h3>
  
  <div class="research-container">
    <!-- ‰∏äÊñπÊñáÊú¨ÈÉ®ÂàÜ -->
    <div class="research-text">
      <p>
        My ongoing research focuses on integrating multimodal large language models (MLLMs) into XR to:
      </p>
      <ol>
        <li>Seamlessly interpret, understand, and respond to users' environments and needs in real-time, enabling XR to support complex, collaborative, and everyday multi-user tasks.</li>
        <li>Transform interactive educational content into dynamic, personalized, and memorable simulation-based learning experiences through AI-driven, adaptive instruction.</li>
      </ol>
    </div>
    
    <!-- ‰∏ãÊñπÂõæÁâáÈÉ®ÂàÜ -->
    <div class="research-image">
      <img src="assets/Research Vision.png" alt="Dynamic but Balanced Human-Human-AI Collaboration" />
    </div>
  </div>
</section>


   <section>
  <h3>Publications</h3>
  <p><em>* denotes equal contribution. ‚ÄúXiuqi Zhu‚Äù and ‚ÄúXiuqi Tommy Zhu‚Äù refer to the same author.</em></p>

  <h4>Peer-reviewed Full Conference and Journal Publications</h4>
<div class="pub">
    <img src="assets/ijhci2024_blindcollege.avif" class="pub-img" alt="Blind and Low Vision Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Understanding the Practice, Perception, and Challenge of Blind or Low Vision Students Learning through Accessible Technologies in Non-Inclusive ‚ÄòBlind College‚Äô</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Ziyue Qiu, Ye Wei, Jianhao Wang, Yang Jiao
      </div>
      <em>International Journal of Human Computer Interaction (2025)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>In developing and underdeveloped regions, many 'Blind Colleges' exclusively enroll individuals with Blindness or Vision Impairment (BLV) for higher education. While advancements in accessible technologies have facilitated BLV student integration into 'Integrated Colleges,' their implementation in 'Blind Colleges' remains uneven due to complex economic, social, and policy challenges. This study investigates the practices, perceptions, and challenges of BLV students using accessible technologies in a Chinese 'Blind College' through a two-part empirical approach. Our findings demonstrate that tactile and digital technologies enhance access to education but face significant integration barriers. We emphasize the critical role of early education in addressing capability gaps, BLV students' aspirations for more inclusive educational environments, and the systemic obstacles within existing frameworks. We advocate for leveraging accessible technologies to transition 'Blind Colleges' into 'Integrated Colleges,' offering actionable insights for policymakers, designers, and educators. Finally, we outline future research directions on accessible technology innovation and its implications for BLV education in resource-constrained settings.</p>
      </div>
      <p><a href="https://doi.org/10.1080/10447318.2025.2526569" target="_blank">[DOI]</a></p>
    </div>
  </div>
     
  <div class="pub">
    <img src="assets/ismar2023_moveit.avif" class="pub-img" alt="Can You Move It Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Can You Move It?: The Design and Evaluation of Moving VR Shots in Sports Broadcast</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Zhu</strong>, Cenyi Wang, Zichun Guo, Yifan Zhao, Yang Jiao
      </div>
      <em>IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2023)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>Virtual Reality (VR) broadcasting has seen widespread adoption in major sports events, attributed to its ability to generate a sense of presence, curiosity, and excitement among viewers. However, we have noticed that still shots reveal a limitation in the movement of VR cameras and hinder the VR viewing experience in current VR sports broadcasts. This paper aims to bridge this gap by engaging in a quantitative user analysis to explore the design and impact of dynamic VR shots on viewing experiences. We conducted two user studies in a digital hockey game twin environment and asked participants to evaluate their viewing experience through two questionnaires. Our findings suggested that the viewing experiences demonstrated no notable disparity between still and moving shots for single clips. However, when considering entire events, moving shots improved the viewer‚Äôs immersive experience, with no notable increase in sickness compared to still shots. We further discuss the benefits of integrating moving shots into VR sports broadcasts and present a set of design considerations and potential improvements for future VR sports broadcasting.</p>
      </div>
      <p><a href="https://ieeexplore.ieee.org/abstract/document/10316354" target="_blank">[DOI]</a></p>
    </div>
  </div>

  <h4>Posters, Extended Abstracts, Workshop Papers and Technical Reports</h4>
  <div class="pub">
    <img src="assets/chi2025_vr_simulation.avif" class="pub-img" alt="CHI 2025 VR Simulation Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Designing VR Simulation System for Clinical Communication Training with LLMs-Based Embodied Conversational Agents</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Mingxin Cheng, Heidi Cheerman, Sheri Kiami, Leanne Chukoskie, Eileen McGivney
      </div>
      <em>Conference on Human Factors in Computing Systems (CHILBW 2025)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>VR simulation in Health Professions (HP) education demonstrates huge potential, but fixed learning content with little customization limits its application beyond lab environments. To address these limitations in the context of VR for patient communication training, we conducted a user-centered study involving semi-structured interviews with advanced HP students to understand their challenges in clinical communication training and perceptions of VR-based solutions. From this, we derived design insights emphasizing the importance of realistic scenarios, simple interactions, and unpredictable dialogues. Building on these insights, we developed the Virtual AI Patient Simulator (VAPS), a novel VR system powered by Large Language Models (LLMs) and Embodied Conversational Agents (ECAs), supporting dynamic and customizable patient interactions for immersive learning. We also provided an example of how clinical professors could use user-friendly design forms to create personalized scenarios that align with course objectives in VAPS and discuss future implications of integrating AI-driven technologies into VR education.</p>
      </div>
      <p><a href="https://dl.acm.org/doi/full/10.1145/3706599.3719693">[DOI]</a></p>
    </div>
  </div>

  <div class="pub">
    <img src="assets/idc2023_cospace.avif" class="pub-img" alt="Co-Space Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Co-Space: A Tangible System Supporting Social Attention and Social Behavioral Development through Embodied Play</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Zhu</strong>, Min Fan, Zhuohao Wu, Jiayi Lu, Yukai Liu
      </div>
      <em>Interaction Design Children (IDC 2023)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>Early impairments in social attention prevent children with autism spectrum disorder (ASD) from acquiring social information and disrupt their social behavior development. We present Co-Space, a tangible system that supports children with ASD aged 4-8 years in developing social attention and social behaviors through collaborative and embodied play in a classroom setting. Children can paint tangible hemispheres, insert them on a projection board, and then press or rotate the hemispheres independently or collaboratively to view dynamic audiovisual feedback. We leveraged the strengths of children with ASD (e.g., who may have visual strengths and prefer repetitive actions) by providing them with tangible hemispheres to engage their participation. We utilized codependent design and dynamic audiovisual cues to facilitate children‚Äôs social attention and encourage their collaboration and potential social behaviors. We demonstrated multiple ways to integrate the tangible system into classroom activities. We discuss the benefits of designing tangible systems for supporting social attention and social behaviors for children with ASD through play in the classroom.</p>
      </div>
      <p><a href="https://doi.org/10.1145/3585088.3593911" target="_blank">[DOI]</a></p>
    </div>
  </div>

  <div class="pub">
    <img src="assets/hcii2022_sounds.avif" class="pub-img" alt="Natural Sounds Library Figure" />
    <div class="pub-text">
      <h5 class="pub-title">An Initial Attempt to Build a Natural Sounds Library based on Heuristic Evaluation</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Zhu</strong>, Jingyu Zhang, Tongyang Liu, Gang He
      </div>
      <em>Human Computer Interaction International Poster 2022</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>Attention restoration theory (ART) predicts that the natural environment can restore consumed attentional resources. Previous studies also found presenting natural scenes visually can also have such an effect but whether natural sounds may also have this effect has not been fully examined. In this study, we used an exploratory approach to build a library of natural sounds. We surveyed 204 people by asking them to name ten different types of ‚Äònatural‚Äô sounds and the ten types of ‚Äòrelaxing‚Äô sounds. The collected more than 1,800 answers were then coded according to the source and the characteristics. Finally, twenty-one categories of sounds emerged from these responses. Among them, six categories were considered to be both relaxing and natural (e.g. birds‚Äô songs). For other categories, they were only natural (e.g. thunder) or only relaxing (e.g. music). We discussed how to use this sound library in future studies.</p>
      </div>
      <p><a href="https://doi.org/10.1007/978-3-031-19679-9_90" target="_blank">[DOI]</a></p>
    </div>
  </div>

<h4>Under-review Manuscripts and In Preparation</h4>
      <div class="pub">
  <img src="assets/Figure_Visual Breakdown.png" class="pub-img" alt="Poster figure: Conversational successes and breakdowns in smart glasses use" />
  <div class="pub-text">
    <h5 class="pub-title">Conversational Successes and Breakdowns in Everyday Smart Glasses Use</h5>

    <div class="pub-authors">
      <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Xiaoan Liu, Casper Harteveld, Smit Desai, Eileen McGivney
    </div>

    <em>Poster Submission, CHI 2026</em>

    <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
      <p>
        Non-display smart glasses combine first-person visual sensing with voice-only interaction, yet there is limited empirical understanding
        of how they support everyday activities in real-world contexts. This poster examines conversational successes and breakdowns in
        everyday smart glasses use through a month-long collaborative autoethnography (n=2). We identify recurring patterns in interaction
        breakdowns, particularly around interaction repair and progressive sensemaking when users hold partial or evolving knowledge.
        By contrasting these experiences with prior work on voice-only interfaces, we highlight unique affordances and limitations of smart
        glasses and discuss implications for designing future smart glasses systems with or without visual augmentation.
      </p>
    </div>
    <p>
      <a href="https://drive.google.com/file/d/1pbJX971UbFy3mmdKNa1pnDvMnRsuP3Bb/view?usp=drive_link" target="_blank" rel="noopener noreferrer">[PDF]</a>
    </p>
  </div>
</div>


<div class="pub">
        <img src="assets/Creation.png" class="pub-img" alt="Co-Space Figure" />
        <div class="pub-text">
          <h5 class="pub-title">Reimagining the Future of Smart Glasses in Everyday Contexts</h5>
          <div class="pub-authors">
            <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Xiaoan Liu, Casper Harteveld, Smit Desai, Eileen McGivney
          </div>
          <em>In Submission, DIS 2026</em>
          <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
            <p>Current non-display smart glasses lack empirical understandings of how they support everyday activities through first-person vision
sensing and voice-only interaction. This paper aims to examine whether and how the current non-display smart glasses did well and
investigate how such experiences can inform implications for future smart glasses (whether or not with visual augmentation) design.
Adopting a two-step approach, we first conducted a collaborative autoethnography (n=2) to get first-hands understanding of in-
the-wild experiences, especially successfully or unsuccessfully supported everyday activities. Based on these insights, the following
participatory speculation design study (n=8) was conducted in which designers engaged in complex activities to reimagine future
smart glasses. Our findings suggest the interactional limits lie in interaction repair and progressive sensemaking when users hold
partial or evolving knowledge. We then provide design implications that visual augmentation should not be assumed by default in
interaction, with flexibility and dynamic insights when AR is necessary in a long-term integration process..</p>
          </div>
          <p><a href="https://drive.google.com/file/d/1qxEyQd8ze5xXsfdcen1AvvFfi2gDkfIk/view?usp=sharing" target="_blank">[PDF]</a></p>
        </div>
      </div>

      <div class="pub">
        <img src="assets/chi2026_VAPS.jpg" class="pub-img" alt="Co-Space Figure" />
        <div class="pub-text">
          <h5 class="pub-title">"It doesn't look real, but it did feel real.": Designing and Evaluating an LLM-Powered VR Simulation for Health Profession Education</h5>
          <div class="pub-authors">
            <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Heidi Cheerman, Mingxin Cheng, Sheri Kiami, Leanne Chukoskie, Eileen McGivney
          </div>
          <em>In Submission, AIED 2026</em>
          <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
            <p>Clinical communication is essential for health professions
(HP) students, but opportunities to practice before reaching the clinic
are limited to resource-intensive simulation labs or technologies that fail
to capture the unpredictability of real patient encounters. We present
VAPS, a Virtual AI Patient Simulator that integrates embodied conver-
sational agents with Large Language Models (LLMs) to support practice
with dynamic patient interactions. In this paper, we describe the design
process to develop VAPS and report the findings of an exploratory pilot
study with 40 HP students from five disciplines. Results from the pilot
suggest VAPS was usable and reduced students‚Äô nervousness about in-
teracting with patients. Students attributed the unpredictable nature of
LLM-generated conversations to creating a sense of realism and evoking
authentic emotions, yet noted that the patient did not look real. Students
described VAPS as a tool to augment their current training to gain prac-
tice, rather than replacing physical simulations and other instruction.
The findings point to design recommendations for AI-enabled simula-
tions that prioritize simple interactions and combine the affordances of
VR and LLMs to give students accessible, unpredictable practice ways.</div>
          <p><a href="https://drive.google.com/file/d/13jH60iWj-oin5TphAM9k-qJbf1bRKAOm/view?usp=drive_link" target="_blank">[PDF]</a></p>
        </div>
      </div>


     <div class="pub">
        <img src="assets/Teaser.jpg" class="pub-img" alt="Co-Space Figure" />
        <div class="pub-text">
          <h5 class="pub-title">Reshaping Inclusive Interpersonal Dynamics through Smart Glasses in
Mixed-Vision Social Activities</h5>
          <div class="pub-authors">
           Yumo Zhang*, Jieqiong Ding*,  <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Kaige Yang, Yuqing Wei, Shiyi Wang, Yishan Liu, Yang Jiao
          </div>
          <em>In Submission DIS 2026</em>
          <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
            <p>Meaningful social interaction is vital to well-being, yet Blind and Low Vision (BLV) individuals face persistent barriers when collab-
orating with sighted peers due to inaccessible visual cues. While most wearable assistive technologies emphasize individual tasks,
smart glasses introduce opportunities for real-time, contextual support in social settings. To explore how smart glasses affect the
interpersonal dynamics and support inclusion in mixed-vision groups, we developed a smart glasses‚Äìbased system CollabLens as a
technology probe, and employed it in four workshop sessions. We found that smart glasses can meaningfully support inclusive collab-
oration and provide users with greater flexibility and control. However, while sighted participants viewed smart glasses as a medium
reshaping how assistance was negotiated with great potential, BLV participants primarily valued smart glasses for independent task
completion compared to social purposes. We concluded by discussing and synthesizing challenges and opportunities for designing
smart glasses that foster natural social dynamics in future inclusive settings. </div>
          <p><a href="https://drive.google.com/file/d/1rGt4XQqhbJiYx7yEVBXPU0vJEmimjbrt/view?usp=drive_link" target="_blank">[PDF]</a></p>
        </div>
      </div>
    </div>
  </div>
</body>
</html>
