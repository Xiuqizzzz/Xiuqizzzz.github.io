<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tommy (Xiuqi) Zhu</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<!-- ÂØºËà™Ê†è -->
<nav class="topnav">
  <div class="nav-container">
    <div class="nav-logo">
      <a href="index.html">Tommy Zhu</a>
    </div>
    <div class="nav-links">
      <a href="index.html" class="active">About</a>
      <a href="cv.html">CV <span class="download-icon">üìÑ</span></a>
      <a href="blog.html">Blog</a>
    </div>
  </div>
</nav>
  <div class="layout">
    <div class="sidebar">
      <img src="assets/profile.jpg" class="profile-img" alt="Tommy Zhu" />
      <div class="social-section">
        <a href="https://scholar.google.com/citations?user=your_google_scholar_id" target="_blank"><img src="assets/icon_google.png" class="icon" alt="Google Scholar" /></a>
        <a href="https://www.linkedin.com/in/your_linkedin" target="_blank"><img src="assets/icon_linkedin.png" class="icon" alt="LinkedIn" /></a>
        <a href="https://twitter.com/your_username" target="_blank"><img src="assets/icon_x.png" class="icon" alt="Twitter/X" /></a>
        <a href="mailto:your_email@example.com"><img src="assets/icon_email.png" class="icon" alt="Email" /></a>
      </div>
      <div class="news-section">
        <h3>News</h3>
        <ul>
          <li>üì¢ 06/2025 ‚Äî One First-author Paper accepted by IJHCI, good research takes time!</li>
          <li>üì¢ 04/2025 ‚Äî Attending my very second CHI conference in Yokohama!</li>
          <li>üì¢ 02/2025 ‚Äî First-author paper accepted by CHI LBW, see you in Yokohama!</li>
          <li>üìÑ 01/2025 ‚Äî Submitted three papers to DIS, CHI LBW, and IJHCI</li>
        </ul>
      </div>
    </div>

    <div class="main-content">
      <h1>Tommy (Xiuqi) Zhu</h1>
      <h2>Ph.D. Student @ Northeastern University</h2>

      <p class="bio">
        I am a Second-Year Ph.D. student at 
        <a href="https://camd.northeastern.edu/" target="_blank">Interdisplinary Media and Design @Northeastern CAMD</a>, 
        advised by 
        <a href="https://camd.northeastern.edu/faculty/eileen-mcgivney/" target="_blank">Prof. Eileen McGivney</a>.
        My research interest lies in the interdisciplinary area of Human-Computer Interaction, 
        with the intersection of Multimodal Large Language Models (LLMs), Extended Reality, and Education Computing.
        I have been honored to do research with various amazing advisors, such as the 
        <a href="https://scholar.google.com/citations?user=nifMhLoAAAAJ&hl=en" target="_blank">Prof. Yang Jiao</a>, 
        <a href="https://thfl.tsinghua.edu.cn/en/yjdw/jzg/Central_Organization/Human_Computer_Interaction__and_User_Experience/Resercher/Yingqing_Xu.htm" target="_blank">Prof. Yingqing Xu</a>, 
        <a href="https://scholar.google.com/citations?user=59HM5VgAAAAJ&hl=en" target="_blank">Prof. Jingyu Zhang</a>, 
        and <a href="https://scholar.google.com/citations?user=r43qpoUAAAAJ&hl=en" target="_blank">Prof. Min Fan</a>.
        I have also interned at <a href="https://www.larksuite.com/" target="_blank">Lark Suite-People</a> as a UX researcher. 
        I received my bachelor's degree in Digital Media Arts from the 
        <a href="https://www.cuc.edu.cn/" target="_blank">Communication University of China</a>. 
        My works have won numerous awards in art and design competitions and have been exhibited in multiple art galleries.
      </p>

<!-- Á†îÁ©∂ÂÖ¥Ë∂£ÈÉ®ÂàÜ -->
<section>
  <h3>Research Interests</h3>
  
  <div class="research-container">
    <!-- ‰∏äÊñπÊñáÊú¨ÈÉ®ÂàÜ -->
    <div class="research-text">
      <p>
        My ongoing research focuses on integrating multimodal large language models (MLLMs) into XR to:
      </p>
      <ol>
        <li>Seamlessly interpret, understand, and respond to users' environments and needs in real-time, enabling XR to support complex, collaborative, and everyday multi-user tasks.</li>
        <li>Transform interactive educational content into dynamic, personalized, and memorable simulation-based learning experiences through AI-driven, adaptive instruction.</li>
      </ol>
    </div>
    
    <!-- ‰∏ãÊñπÂõæÁâáÈÉ®ÂàÜ -->
    <div class="research-image">
      <img src="assets/research-illustration.png" alt="AI-Enhanced XR Educational Experience" />
    </div>
  </div>
</section>


   <section>
  <h3>Publications</h3>
  <p><em>* denotes equal contribution. ‚ÄúXiuqi Zhu‚Äù and ‚ÄúXiuqi Tommy Zhu‚Äù refer to the same author.</em></p>

  <h4>Peer-reviewed Full Conference and Journal Publications</h4>
<div class="pub">
    <img src="assets/ijhci2024_blindcollege.avif" class="pub-img" alt="Blind and Low Vision Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Understanding the Practice, Perception, and Challenge of Blind or Low Vision Students Learning through Accessible Technologies in Non-Inclusive ‚ÄòBlind College‚Äô</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Ziyue Qiu, Ye Wei, Jianhao Wang, Yang Jiao
      </div>
      <em>International Journal of Human Computer Interaction (2025)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>In developing and underdeveloped regions, many 'Blind Colleges' exclusively enroll individuals with Blindness or Vision Impairment (BLV) for higher education. While advancements in accessible technologies have facilitated BLV student integration into 'Integrated Colleges,' their implementation in 'Blind Colleges' remains uneven due to complex economic, social, and policy challenges. This study investigates the practices, perceptions, and challenges of BLV students using accessible technologies in a Chinese 'Blind College' through a two-part empirical approach. Our findings demonstrate that tactile and digital technologies enhance access to education but face significant integration barriers. We emphasize the critical role of early education in addressing capability gaps, BLV students' aspirations for more inclusive educational environments, and the systemic obstacles within existing frameworks. We advocate for leveraging accessible technologies to transition 'Blind Colleges' into 'Integrated Colleges,' offering actionable insights for policymakers, designers, and educators. Finally, we outline future research directions on accessible technology innovation and its implications for BLV education in resource-constrained settings.</p>
      </div>
      <p><a href="" target="_blank">[arXiv]</a></p>
    </div>
  </div>
     
  <div class="pub">
    <img src="assets/ismar2023_moveit.avif" class="pub-img" alt="Can You Move It Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Can You Move It?: The Design and Evaluate of Moving Shots in VR Sports Broadcast</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Zhu</strong>, Cenyi Wang, Zichun Guo, Yifan Zhao, Yang Jiao
      </div>
      <em>IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2023)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>Virtual Reality (VR) broadcasting has seen widespread adoption in major sports events, attributed to its ability to generate a sense of presence, curiosity, and excitement among viewers. However, we have noticed that still shots reveal a limitation in the movement of VR cameras and hinder the VR viewing experience in current VR sports broadcasts. This paper aims to bridge this gap by engaging in a quantitative user analysis to explore the design and impact of dynamic VR shots on viewing experiences. We conducted two user studies in a digital hockey game twin environment and asked participants to evaluate their viewing experience through two questionnaires. Our findings suggested that the viewing experiences demonstrated no notable disparity between still and moving shots for single clips. However, when considering entire events, moving shots improved the viewer‚Äôs immersive experience, with no notable increase in sickness compared to still shots. We further discuss the benefits of integrating moving shots into VR sports broadcasts and present a set of design considerations and potential improvements for future VR sports broadcasting.</p>
      </div>
      <p><a href="https://ieeexplore.ieee.org/abstract/document/10316354" target="_blank">[DOI]</a></p>
    </div>
  </div>

  <h4>Posters, Extended Abstracts, Workshop Papers and Technical Reports</h4>
  <div class="pub">
    <img src="assets/chi2025_vr_simulation.avif" class="pub-img" alt="CHI 2025 VR Simulation Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Designing VR Simulation System for Clinical Communication Training with LLMs-Based Embodied Conversational Agents</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Tommy Zhu</strong>, Mingxin Cheng, Heidi Cheerman, Sheri Kiami, Leanne Chukoskie, Eileen McGivney
      </div>
      <em>Conference on Human Factors in Computing Systems (CHILBW 2025)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>VR simulation in Health Professions (HP) education demonstrates huge potential, but fixed learning content with little customization limits its application beyond lab environments. To address these limitations in the context of VR for patient communication training, we conducted a user-centered study involving semi-structured interviews with advanced HP students to understand their challenges in clinical communication training and perceptions of VR-based solutions. From this, we derived design insights emphasizing the importance of realistic scenarios, simple interactions, and unpredictable dialogues. Building on these insights, we developed the Virtual AI Patient Simulator (VAPS), a novel VR system powered by Large Language Models (LLMs) and Embodied Conversational Agents (ECAs), supporting dynamic and customizable patient interactions for immersive learning. We also provided an example of how clinical professors could use user-friendly design forms to create personalized scenarios that align with course objectives in VAPS and discuss future implications of integrating AI-driven technologies into VR education.</p>
      </div>
      <p><a href="https://dl.acm.org/doi/full/10.1145/3706599.3719693">[DOI]</a></p>
    </div>
  </div>

  <div class="pub">
    <img src="assets/idc2023_cospace.avif" class="pub-img" alt="Co-Space Figure" />
    <div class="pub-text">
      <h5 class="pub-title">Co-Space: A Tangible System Supporting Social Attention and Social Behavioral Development through Embodied Play</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Zhu</strong>, Min Fan, Zhuohao Wu, Jiayi Lu, Yukai Liu
      </div>
      <em>Interaction Design Children (IDC 2023)</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>Early impairments in social attention prevent children with autism spectrum disorder (ASD) from acquiring social information and disrupt their social behavior development. We present Co-Space, a tangible system that supports children with ASD aged 4-8 years in developing social attention and social behaviors through collaborative and embodied play in a classroom setting. Children can paint tangible hemispheres, insert them on a projection board, and then press or rotate the hemispheres independently or collaboratively to view dynamic audiovisual feedback. We leveraged the strengths of children with ASD (e.g., who may have visual strengths and prefer repetitive actions) by providing them with tangible hemispheres to engage their participation. We utilized codependent design and dynamic audiovisual cues to facilitate children‚Äôs social attention and encourage their collaboration and potential social behaviors. We demonstrated multiple ways to integrate the tangible system into classroom activities. We discuss the benefits of designing tangible systems for supporting social attention and social behaviors for children with ASD through play in the classroom.</p>
      </div>
      <p><a href="https://doi.org/10.1145/3585088.3593911" target="_blank">[DOI]</a></p>
    </div>
  </div>

  <div class="pub">
    <img src="assets/hcii2022_sounds.avif" class="pub-img" alt="Natural Sounds Library Figure" />
    <div class="pub-text">
      <h5 class="pub-title">An Initial Attempt to Build a Natural Sounds Library based on Heuristic Evaluation</h5>
      <div class="pub-authors">
        <strong class="highlight-author">Xiuqi Zhu</strong>, Jingyu Zhang, Tongyang Liu, Gang He
      </div>
      <em>Human Computer Interaction International Poster 2022</em>
      <div class="pub-abstract" style="max-height: 160px; overflow-y: auto; margin-top: 8px;">
        <p>Attention restoration theory (ART) predicts that the natural environment can restore consumed attentional resources. Previous studies also found presenting natural scenes visually can also have such an effect but whether natural sounds may also have this effect has not been fully examined. In this study, we used an exploratory approach to build a library of natural sounds. We surveyed 204 people by asking them to name ten different types of ‚Äònatural‚Äô sounds and the ten types of ‚Äòrelaxing‚Äô sounds. The collected more than 1,800 answers were then coded according to the source and the characteristics. Finally, twenty-one categories of sounds emerged from these responses. Among them, six categories were considered to be both relaxing and natural (e.g. birds‚Äô songs). For other categories, they were only natural (e.g. thunder) or only relaxing (e.g. music). We discussed how to use this sound library in future studies.</p>
      </div>
      <p><a href="https://doi.org/10.1007/978-3-031-19679-9_90" target="_blank">[DOI]</a></p>
    </div>
  </div>

  <h4>Under-review Manuscripts, Work-in-Progress and Preparation</h4>
</section>

    </div>
  </div>
</body>
</html>
